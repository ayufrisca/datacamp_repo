{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayufrisca/datacamp_repo/blob/master/Selamat_Datang_di_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install the scrapy\n",
        "$ pip install scrapy\n",
        "#install the image for downloading the product images\n",
        "$ pip install image\n",
        "#start web scraping project with scraps\n",
        "$ scrapy startproject fashionWebScraping\n",
        "$ cd fashionWebScraping\n",
        "$ ls\n",
        "#create project folders which are explained below\n",
        "$ mkdir csvFiles\n",
        "$ mkdir images_scraped\n",
        "$ mkdir jsonFiles\n",
        "$ mkdir utilityScripts\n"
      ],
      "metadata": {
        "id": "ana72wVxwCFH",
        "outputId": "342725cc-d3e1-4a25-cddd-b5c0c098adfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-07e33ff731a0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    $ pip install scrapy\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.derimod.com.tr/kadin-canta-aksesuar/?p=1"
      ],
      "metadata": {
        "id": "GBoR8q9DwDnJ",
        "outputId": "8f2aec2c-3627-44d5-e611-00f576845448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-fc4229ae6813>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://www.derimod.com.tr/kadin-canta-aksesuar/?p=1\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#items.py in fashionWebScraping folder\n",
        "import scrapy\n",
        "from scrapy.item import Item, Field\n",
        "class FashionwebscrapingItem(scrapy.Item):\n",
        " \n",
        " #product related items, such as id,name,price\n",
        " gender=Field()\n",
        " productId=Field()\n",
        " productName=Field()\n",
        " priceOriginal=Field()\n",
        " priceSale=Field()\n",
        "#items to store links\n",
        " imageLink = Field()\n",
        " productLink=Field()\n",
        "#item for company name\n",
        " company = Field()\n",
        "pass\n",
        "class ImgData(Item):\n",
        "#image pipline items to download product images\n",
        " image_urls=scrapy.Field()\n",
        " images=scrapy.Field()"
      ],
      "metadata": {
        "id": "ntzasC_gwpjq",
        "outputId": "3a244dcc-cca6-49ea-f57c-289f830a9cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0339e25d982a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#items.py in fashionWebScraping folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mItem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFashionwebscrapingItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# settings.py in fashionWebScraping folder\n",
        "# Scrapy settings for fashionWebScraping project\n",
        "# For simplicity, this file contains only settings considered important or commonly used. You can find more settings consulting the documentation:\n",
        "# https://doc.scrapy.org/en/latest/topics/settings.html\n",
        "# https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "# https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "BOT_NAME = 'fashionWebScraping'\n",
        "SPIDER_MODULES = ['fashionWebScraping.spiders']\n",
        "NEWSPIDER_MODULE = 'fashionWebScraping.spiders'\n",
        "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
        "USER_AGENT = 'fashionWebScraping'\n",
        "# Obey robots.txt rules\n",
        "ROBOTSTXT_OBEY = True\n",
        "# See https://doc.scrapy.org/en/latest/topics/settings.html\n",
        "# download-delay\n",
        "# See also autothrottle settings and docs\n",
        "# This to avoid hitting servers too hard\n",
        "DOWNLOAD_DELAY = 1\n",
        "# Override the default request headers:\n",
        "DEFAULT_REQUEST_HEADERS = {\n",
        "'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "'Accept-Language': 'tr',\n",
        "}\n",
        "# Configure item pipelines\n",
        "# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}\n",
        "IMAGES_STORE = '/Users/erdemisbilen/Angular/fashionWebScraping/images_scraped'\n",
        "\n"
      ],
      "metadata": {
        "id": "q0NpR0xHwrYd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'fashionBOYNER.py' in fashionWebScraping/Spiders folder\n",
        "# import scrapy and scrapy items\n",
        "import scrapy\n",
        "from fashionWebScraping.items import FashionwebscrapingItem\n",
        "from fashionWebScraping.items import ImgData\n",
        "from scrapy.http import Request\n",
        "# To read from a csv file\n",
        "import csv\n",
        "class FashionboynerSpider(scrapy.Spider):\n",
        " name = 'fashionBOYNER'\n",
        " allowed_domains = ['BOYNER.com']\n",
        " start_urls = ['http://BOYNER.com/']\n",
        "# This function helps us to scrape the whole content of the website\n",
        " # by following the starting URLs in a csv file.\n",
        "def start_requests(self):\n",
        "# Read main category URLs from a csv file\n",
        "with open (\"/Users/erdemisbilen/Angular/fashionWebScraping/\n",
        "  csvFiles/SpiderMainCategoryLinksBOYNER.csv\", \"rU\") as f:\n",
        "  \n",
        "    reader=csv.DictReader(f)\n",
        "for row in reader:\n",
        "      url=row['url']\n",
        "# Change the page value incrementally to navigate through\n",
        "      the product list\n",
        "      # You can play with the range value according to maximum  \n",
        "      product quantity, 30 pages to scrape as default\n",
        "      link_urls = [url.format(i) for i in range(1,30)]\n",
        "for link_url in link_urls:\n",
        "        print(link_url)\n",
        "# Pass the each link containing products to \n",
        "        parse_ product_pages function with the gender metadata\n",
        "request=Request(link_url, callback=self.parse_product_pages,\n",
        "        meta={'gender': row['gender']})\n",
        "yield request\n",
        "# This function scrapes the page with the help of xpath provided\n",
        " def parse_product_pages(self,response):\n",
        " \n",
        "  item=FashionwebscrapingItem()\n",
        "  \n",
        "  # Get the HTML block where all the products are listed\n",
        "  # <div> HTML element with the \"product-list-item\" class name\n",
        "content=response.xpath('//div[starts-with(@class,\"product-list-\n",
        "  item\")]')\n",
        "# loop through the each <div> element in the content\n",
        "  for product_content in content:\n",
        "image_urls = []\n",
        "# get the product details and populate the items\n",
        "   item['productId']=product_content.xpath('.//a/@data\n",
        "   -id').extract_first()\n",
        "item['productName']=product_content.xpath('.//img/@title').\n",
        "   extract_first()\n",
        "item['priceSale']=product_content.xpath('.//ins[@class=\n",
        "   \"price-payable\"]/text()').extract_first()\n",
        "item['priceOriginal']=product_content.xpath('.//del[@class=\n",
        "   \"price-psfx\"]/text()').extract_first()\n",
        "if item['priceOriginal']==None:\n",
        "    item['priceOriginal']=item['priceSale']\n",
        "item['imageLink']=product_content.xpath('.//img/\n",
        "   @data-original').extract_first()\n",
        "   \n",
        "   item['productLink']=\"https://www.boyner.com.tr\"+\n",
        "   product_content.xpath('.//a/@href').extract_first()\n",
        "image_urls.append(item['imageLink'])\n",
        "item['company']=\"BOYNER\"\n",
        "   item['gender']=response.meta['gender']\n",
        "if item['productId']==None:\n",
        "    break\n",
        "yield (item)\n",
        "# download the image contained in image_urls\n",
        "   yield ImgData(image_urls=image_urls)\n",
        "def parse(self, response):\n",
        "  pass"
      ],
      "metadata": {
        "id": "_RMLd6cHw6Gd",
        "outputId": "d64ad624-6278-4f50-f52e-e6fb5b25ae38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-0b9d6647502f>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    with open (\"/Users/erdemisbilen/Angular/fashionWebScraping/\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " // Selects nodes in the document from the current node that matches the selection no matter where they are\n",
        "//div[starts-with(@class,\"product-list-item\")]' selects the all div elements which has class value start\n",
        "content = response.xpath('//div[starts-with(@class,\"product-list-item\")]')"
      ],
      "metadata": {
        "id": "S7i9zplzxN2C",
        "outputId": "f39e5114-267b-4cb7-95a8-ab4b10d48a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-717275ca7437>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    //div[starts-with(@class,\"product-list-item\")]' selects the all div elements which has class value start\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through the each <div> element in the content\n",
        "  for product_content in content:\n",
        "image_urls = []\n",
        "# get the product details and populate the items\n",
        "   \n",
        "   # ('.//a/@data-id') extracts 'data-id' value of <a> element\n",
        "   inside the product_content\n",
        "   item['productId']=product_content.xpath('.//a/@data\n",
        "   -id').extract_first()\n",
        "# ('.//img/@title') extracts 'title' value of <img> element\n",
        "   inside the product_content   \n",
        "   item['productName']=product_content.xpath('.//img/@title').\n",
        "   extract_first()\n",
        "# ('.//ins[@class= \"price-payable\"]/text()') extracts text value\n",
        "   of <ins> element which has 'price-payable' class attribute inside\n",
        "   the product_content   \n",
        "   item['priceSale']=product_content.xpath('.//ins[@class=\n",
        "   \"price-payable\"]/text()').extract_first()\n",
        "# ('.//del[@class=\"price-psfx\"]/text()') extracts text value\n",
        "   of <del> element which has 'price-psfx' class attribute inside\n",
        "   the product_content\n",
        "   item['priceOriginal']=product_content.xpath('.//del[@class=\n",
        "   \"price-psfx\"]/text()').extract_first()\n",
        "if item['priceOriginal']==None:\n",
        "     item['priceOriginal']=item['priceSale']\n",
        "# ('.//img/@data-original') extracts 'data-original' value of\n",
        "   <img> element inside the product_content\n",
        "   item['imageLink']=product_content.xpath('.//img/\n",
        "   @data-original').extract_first()\n",
        "# ('.//a/@href') extracts 'href' value of\n",
        "   <a> element inside the product_content\n",
        "   item['productLink']=\"https://www.boyner.com.tr\"+\n",
        "   product_content.xpath('.//a/@href').extract_first()\n",
        "# assigns the product image link into the 'image_urls' which is\n",
        "   defined in the image pipeline\n",
        "   image_urls.append(item['imageLink'])\n",
        "item['company']=\"BOYNER\"\n",
        "   item['gender']=response.meta['gender']\n",
        "if item['productId']==None:\n",
        "    break\n",
        "yield (item)\n",
        " \n",
        "   # download the image contained in image_urls\n",
        "   yield ImgData(image_urls=image_urls)"
      ],
      "metadata": {
        "id": "kO8p-PpRxWld",
        "outputId": "9772dce0-7042-40bc-c22d-25184d6ae39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    <img> element inside the product_content\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‘jsonPrep.py’ in fashionWebScraping/utilityScripts folder\n",
        "import json\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import csv\n",
        "import os\n",
        "# Reads from jsonFiles.csv file for the names and locations of all json files need to be validated \n",
        "with open(\"/Users/erdemisbilen/Angular/fashionWebScraping/csvFiles/ jsonFiles.csv\", \"rU\") as f:\n",
        "reader=csv.DictReader(f)\n",
        "# Iterates the json files listed in jsonFiles.csv\n",
        " for row in reader:\n",
        " \n",
        "  # Reads from jsonFiles.csv file for jsonFile_raw column\n",
        "  jsonFile=row['jsonFile_raw']\n",
        "# Opens the jsonFile\n",
        "   with open(jsonFile) as json_file:\n",
        "    data = []\n",
        "    i = 0\n",
        "seen = OrderedDict()\n",
        "    \n",
        "    # Iterates in the rows of json file\n",
        "    for d in json_file:\n",
        "     seen = json.loads(d)\n",
        "# Do not include the line item if the product Id is null\n",
        "     try:\n",
        "      if seen[\"productId\"] != None:\n",
        "       for key, value in seen.items():\n",
        "        print(\"ok\")\n",
        "        i = i + 1\n",
        "        data.append(json.loads(d))\n",
        "     \n",
        "     except KeyError:\n",
        "      print(\"nok\")\n",
        "    \n",
        "    print (i)\n",
        "    \n",
        "    baseFileName=os.path.splitext(jsonFile)[0]\n",
        "# Write the result as a json file by reading filename from the\n",
        "    'file_name_prep' column\n",
        "    with open('/Users/erdemisbilen/Angular/fashionWebScraping/\n",
        "    jsonFiles/'+row['file_name_prep'], 'w') as out:\n",
        "json.dump(data, out)"
      ],
      "metadata": {
        "id": "r4N9IPoLxiNH",
        "outputId": "1ece158c-6e86-4b26-b9fe-2cf52debc081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-ab927de00a80>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    with open('/Users/erdemisbilen/Angular/fashionWebScraping/\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'deldub.py' in fashionWebScraping/utilityScripts folder\n",
        "import json\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import csv\n",
        "import os\n",
        "# Reads from jsonFiles.csv file for the names and locations of all json files need to be validated for dublicate lines\n",
        "with open(\"/Users/erdemisbilen/Angular/fashionWebScraping/csvFiles/ jsonFiles.csv\", newline=None) as f:\n",
        " \n",
        " reader=csv.DictReader(f)\n",
        "# Iterates the json files listed in jsonFiles.csv\n",
        " for row in reader:\n",
        "# Reads from jsonFiles.csv file for jsonFile_raw column\n",
        "  jsonFile=row['jsonFile_prep']\n",
        "# Opens the jsonFile\n",
        "  with open(jsonFile) as json_file:\n",
        "   data = json.load(json_file)\n",
        "seen = OrderedDict()\n",
        "   dubs = OrderedDict()\n",
        "# Iterates in the rows of json file\n",
        "   for d in data:\n",
        "    oid = d[\"productId\"]\n",
        "# Don't include the item if the product Id has dublicate value\n",
        "    if oid not in seen:\n",
        "     seen[oid] = d\n",
        "else:\n",
        "     dubs[oid]=d\n",
        "baseFileName=os.path.splitext(jsonFile)[0]\n",
        "# Write the result as a json file by reading filename from the\n",
        "     'file_name_final' column\n",
        "with open('/Users/erdemisbilen/Angular/fashionWebScraping/\n",
        "     jsonFiles/'+row['file_name_final'], 'w') as out:\n",
        "      json.dump(list(seen.values()), out)\n",
        "with open('/Users/erdemisbilen/Angular/fashionWebScraping/\n",
        "     jsonFiles/'+'DELETED'+row['file_name_final'], 'w') as out:\n",
        "      json.dump(list(dubs.values()), out)"
      ],
      "metadata": {
        "id": "aU33zIYvxt0w",
        "outputId": "009f776b-6410-4432-ce25-cda97ffeabe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-b2e5920b21b0>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    dubs = OrderedDict()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "$ python3 --version\n",
        "Python 3.7.3$ virtualenv --version\n",
        "15.2.0$ cd /path/to/my/airflow/workspace\n",
        "$ virtualenv -p `which python3` venv\n",
        "$ source venv/bin/activate\n",
        "(venv) $ pip install apache-airflow\n",
        "(venv) $ mkdir airflow_home\n",
        "(venv) $ export AIRFLOW_HOME=`pwd`/airflow_home\n",
        "(venv) $ airflow initdb\n",
        "(venv) $ airflow webserver"
      ],
      "metadata": {
        "id": "qJOkW1dwyAly",
        "outputId": "4d6ded36-4a85-4f8c-df53-ac77d716f745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-7baa2834df56>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    $ python3 --version\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(venv) $ mkdir dags"
      ],
      "metadata": {
        "id": "msAbAvZUyGrM",
        "outputId": "9fcc1af9-f92f-4f81-f2e5-c040fff939c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-6cade8a88049>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (venv) $ mkdir dags\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'fashionsearch_dag.py' in Airflow dag folder\n",
        "import datetime as dt\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "default_args = {\n",
        "'owner': 'airflow',\n",
        "'depends_on_past': False,\n",
        "'start_date': datetime(2019, 11, 23),\n",
        "'retries': 1,\n",
        "'retry_delay': timedelta(minutes=5),\n",
        "# 'queue': 'bash_queue',\n",
        "# 'pool': 'backfill',\n",
        "# 'priority_weight': 10,\n",
        "# 'end_date': datetime(2016, 1, 1),\n",
        "}\n",
        "dag = DAG(dag_id='fashionsearch_dag', default_args=default_args, schedule_interval=timedelta(days=1))\n",
        "# This task deletes all json files which are generated in previous scraping sessions\n",
        "t1 = BashOperator(\n",
        "task_id='delete_json_files',\n",
        "bash_command='run_delete_files',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.boyner.com\n",
        "# And populates the related json file with data scraped\n",
        "t2 = BashOperator(\n",
        "task_id='boyner_spider',\n",
        "bash_command='run_boyner_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.derimod.com\n",
        "# And populates the related json file with data scraped\n",
        "t3 = BashOperator(\n",
        "task_id='derimod_spider',\n",
        "bash_command='run_derimod_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.hepsiburada.com\n",
        "# And populates the related json file with data scraped\n",
        "t4 = BashOperator(\n",
        "task_id='hepsiburada_spider',\n",
        "bash_command='run_hepsiburada_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.hm.com\n",
        "# And populates the related json file with data scraped\n",
        "t5 = BashOperator(\n",
        "task_id='hm_spider',\n",
        "bash_command='run_hm_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.koton.com\n",
        "# And populates the related json file with data scraped\n",
        "t6 = BashOperator(\n",
        "task_id='koton_spider',\n",
        "bash_command='run_koton_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.lcwaikiki.com\n",
        "# And populates the related json file with data scraped\n",
        "t7 = BashOperator(\n",
        "task_id='lcwaikiki_spider',\n",
        "bash_command='run_lcwaikiki_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.matmazel.com\n",
        "# And populates the related json file with data scraped\n",
        "t8 = BashOperator(\n",
        "task_id='matmazel_spider',\n",
        "bash_command='run_matmazel_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.modanisa.com\n",
        "# And populates the related json file with data scraped\n",
        "t9 = BashOperator(\n",
        "task_id='modanisa_spider',\n",
        "bash_command='run_modanisa_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.morhipo.com\n",
        "# And populates the related json file with data scraped\n",
        "t10 = BashOperator(\n",
        "task_id='morhipo_spider',\n",
        "bash_command='run_morhipo_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.mudo.com\n",
        "# And populates the related json file with data scraped\n",
        "t11 = BashOperator(\n",
        "task_id='mudo_spider',\n",
        "bash_command='run_mudo_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.trendyol.com\n",
        "# And populates the related json file with data scraped\n",
        "t12 = BashOperator(\n",
        "task_id='trendyol_spider',\n",
        "bash_command='run_trendyol_spider',\n",
        "dag=dag)\n",
        "# This task runs the spider for www.yargici.com\n",
        "# And populates the related json file with data scraped\n",
        "t13 = BashOperator(\n",
        "task_id='yargici_spider',\n",
        "bash_command='run_yargici_spider',\n",
        "dag=dag)\n",
        "# This task checks and removes null line items in json files\n",
        "t14 = BashOperator(\n",
        "task_id='prep_jsons',\n",
        "bash_command='run_prep_jsons',\n",
        "dag=dag)\n",
        "# This task checks and removes dublicate line items in json files\n",
        "t15 = BashOperator(\n",
        "task_id='delete_dublicate_lines',\n",
        "bash_command='run_del_dub_lines',\n",
        "dag=dag)\n",
        "# This task populates the remote ES clusters with the data inside the JSON files\n",
        "t16 = BashOperator(\n",
        "task_id='json_to_elasticsearch',\n",
        "bash_command='run_json_to_es',\n",
        "dag=dag)\n",
        "# With sequential executer, all tasks depends on previous task\n",
        "# No paralell task execution is possible\n",
        "# Use local executer at least for paralell task execution\n",
        "t1 >> t2 >> t3 >> t4 >> t5 >> t6 >> t7 >> t8 >> t9 >> t10 >> t11 >> t12 >> t13 >> t14 >> t15 >> t16"
      ],
      "metadata": {
        "id": "6K5PoesTyOi2",
        "outputId": "85675dc9-a48c-408e-dbee-d53afb24eb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-089e160b9419>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 'fashionsearch_dag.py' in Airflow dag folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbash_operator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBashOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Selamat Datang di Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}